![](/assets/DP203/00-cover.png)
## Design and implement data storage (15–20%)
Welcome to the exam prep video series on DP203 data engineering on Microsoft Azure my name is Steve Howard Microsoft technical trainer and I will be taking you through the video series let's review the journey you will go through to achieve this certification 

![](/assets/DP203/01-Certification_journey.png)
let's now review where this exam prep video series it's within your certification journey you may have taken the official Microsoft training either self-paced online or the instructor LED training in addition the free online trainings mentioned as prerequisites will give you the experience you need to be successful in this area you should also have hands on experience in enterprise networking on premises or cloud infrastructure and network security and understand on premises virtualization technologies including VMS virtual networking and virtual hard disks network configurations including TCP IP domain name systems virtual private networks firewalls and encryption technologies software defined networking hybrid network connectivity methods resilience and disaster recovery including high availability and restore operations then comes this video series the official exam prep this set of videos is meant to help prepare you to take the exam by covering what types of questions you may see what topics may be covered and what resources you can use to study following these videos you should plan to study the topics you identified during your viewing there are a variety of resources available to you including labs documentation practice tests and more on the video page you will find links to a number of resources such as the study guide for the exam after your self study you should proceed with taking the exam so 

![](/assets/DP203/02-Objectives.png)
let's now discuss the objectives of this video series this exam prep video series will help you #1 identify what will be covered on the exam #2 determine which concepts to prioritize for further study #3 explained the different potential exam question formats and #4 locate available learning resources to utilize remember this video series is not a replacement for the Microsoft official training or your experience working in this space this series will help you learn what you can expect to see on the exam but it won't teach you the topics and concepts and detail you should expect to spend time following this series studying any areas you identify where you need more work let's now review the objective domain of the DP 203 exam 

![](/assets/DP203/03-Objectives_domain_review.png)
objective domains are the primary categories of subjects and ideas that the test covers 

![](/assets/DP203/04-Objectives_domain_breakdown.png)
let's look into the DP 203 objective domain together this exam has three functional groups design and implement data storage develop data processing secure monitoring and optimized data storage and data processing you will see the relative weighting of each group meaning what percentage of the questions will be on which functional groups you may note that functional group 2 develop data processing has the highest weight in the DP 203 exam also note that while certain groups may have a higher weighting you will still need to be familiar with all of them 

![](/assets/DP203/05-Skills_Measured1.png)
let's get started with the first functional group or skills measured our first functional group is about how to design and implement data storage within this group we will learn how to implement a partition strategy and design and implement the data exploration layer 

![](/assets/DP203/06-Implement_a_partition_Strategy_for_files.png)
let's begin with the first topic in this functional group every workload has different requirements on how the data is consumed but these are some common layouts to consider when working with Internet of Things or IO T batch scenarios or when optimizing for time series data in io T workloads there could be a great deal of data ingested that spans across numerous products devices organizations and customers it's important to pre plan the directory layout for organization security and efficient processing of the data for downstream consumers a general template you may consider might be one that uses region as one level of directory within that you might have subject matters as subdirectories then you may have sub directories for each year and a four digit year format then within each of those a 2 digit month format for each month and then a two digit day format for each day and possibly even a 2 digit hour format and then you would place the data coming in into the appropriate directory for batch processing job structures there are a lot of similarities a commonly used approach in batch processing is to place data into an in directory so in such a case you would keep your region and subject matters but then you may have an in directory and then from there you may have the four digit year 2 digit month 2 digit day and two digit hour subdirectories for your out data you may also go regions subject matters and then out and then the formats in the same formats that you've used before and you will always need to deal with some corrupt or bad data so in a case like this we may also go region and subject matters but then have a third subdirectory for bad and then the date format after that for time series data partition pruning can help some queries read only a subset of the data which can improve performance so a common example we see for data that is structured by date may be a data set by name add data 4 digit year 2 digit month to digit day and then your files within that particular subdirectory consider using data flows and Azure data factory or using synapse pipelines to partition data when writing files to Azure data lake storage Gen. 2 or to Azure BLOB storage 

next let's examine how to implement a partition strategy for analytical workloads event hubs use partitions that help to segment data and allow you to read each partition in parallel event hubs stream data at a massive scale partitioning is built into event hubs to enable horizontal scaling you can only read a specific partition of the message stream at a time the event publishers are only aware of the partition key not the actual partition to which the events are being published the decoupling of key and partition prevents the sender from knowing the downstream processing 

next let's examine how to implement a partition strategy for Azure synapse analytics table partitioning allows you to split data into smaller subsets based on a partition column this can help you manage data more easily and improve query performance by reducing the amount of data scanned however the benefits of partitioning depend on how you load data and whether you use the same column for partitioning and filtering remember that you can only partition the table by one column in SQL Server and the partition column must have multiple values upon which to partition partitioning is supported on all table types arrange partitioning creates one or more table partitions these partitions are horizontal table slices that let you apply operations to subsets of rows regardless of whether the table is stored as a heap clustered index or clustered columnstore index the primary benefits to partitioning are that it improves efficiency performance of of loading and querying by limiting the scope to a subset of data it may offer query performance enhancements where filtering on the partition key can eliminate unnecessary scans and eliminate input and output operations and with multiple column distribution or MCD which is only available on dedicated SQL pools you can choose up to 8 columns for distribution MCD is highly desirable for easing migrations and it promotes faster query performance and reduces data skew while partitioning can be used to improve performance creating a table with too many partitions can hurt performance so for partitioning to be helpful it's important to understand when to use partitioning and the number of partitions to create a successful partitioning scheme usually has 10s to hundreds of partitions and not thousands 

next let's examine how to identify when partitioning is needed in Azure data lake storage Gen. 2 partitioning can improve scalability reduce contention and optimize performance the partitioning strategy must be chosen carefully to maximize the benefits while minimizing adverse effects consider scale targets for BLOB storage and storage accounts partitioning as your BLOB storage will mean that the partition key that you choose is going to be your account name plus your container name plus the BLOB name the partition key is used to partition data into ranges and these ranges are load balanced across the system blobs are distributed across many servers to scale out access to them a single BLOB can only be served by a single server the naming convention that uses timestamps or numerical identifiers can lead to a hot partition which limits the system from effectively balancing your load so for example if you have daily operations that use a BLOB object with a timestamp such as the four digit year dash 2 digit month dash 2 digit day all the traffic for that operation goes to a single partition server so consider prefixing the name with a three digit hash writing a single block or page is an atomic action which means it either succeeds or fails as a whole however writing across multiple blocks pages or blobs is not atomic which means it can result in partial or inconsistent data to ensure data consistency when performing write operations is spanned blocks pages or blobs you need to acquire a BLOB lease that locks the BLOB and prevents other writers from modifying it 

next let's examine how to create and execute queries by using a compute solution as your synapse serverless SQL pool is tailored for querying the data in the data link it supports querying formats such as CSV JSON and parquet file formats directly CSV files are a common file format within many businesses and you can query a single CSV file using serverless SQL pool so when we query these various formats some of the formats we can specify include with or without a header row, or tab delimited values windows or Unix style index non quoted and quoted values and escaping characters the open roset function works on the serverless SQL pool but not the dedicated SQL pool and enables you to read the content of RK file by providing the URL to your file the query should use open row set to reach Jason files of two formats either standard JSON files that contain a JSON array of multiple JSON documents or align delimited JSON file that separates each JSON document with a new line character these files usually have extensions like JSON L or Lt. JSON or NDJ son you can also query multiple files and folders using Azure synapse serverless SQL pools the SQL serverless supports reading multiple files or folders by using wildcards similar to wild cards that are used in the Windows operating system 

next let's examine how to implement Azure synapse analytics database templates you can implement Azure snaps analytics database templates to create a linked database to align data to a new model and use the integrated experience to analyze the data the steps to implement an Azure synapse analytics database template are to create the lake database from database templates using a new database templates functionality and then number one we will configure a lake database by making sure that the storage account and the file path are set to a location where you wish to destroy the data #2 and just data to lake database by executing pipelines with code free data flow mapping that have workspace DB connector to load data directly to the database table #3 query the data by opening a new spark notebook and using the integrated experience note that SQL databases in serverless SQL pools are supported and automatically understand the newly created late database format and #4 you can train machine learning models using the like database 

next let's examine how to recommend as your synapse analytics database templates a typical database template addresses the core requirements of a specific industry and consists of one or more enterprise templates and tables grouped by business areas you can leverage the library of Azure synapse analytics database templates to accelerate time to insights based on the standardized business area plans for different industries to identify gaps and opportunities in the existing enterprise data model or to consolidate data silos and query from synapse studio or to create a well formed data lake ready for analytics at scale to enrich data with Azure cognitive services and Azure machine learning or to develop reports using power BI 

next let's examine how to push new or updated data lineage from Microsoft per view the data factory user interface is used to create a pipeline that runs activities and reports lineage data to the Microsoft purview account that data lineage information can be viewed in the Microsoft purview account in order to view this in order to set this up you have performed the following steps to push new or updated data lineage to Microsoft per view number one connect data factory to the Microsoft purview account you can do that with two options you can connect data factory to Microsoft per view account in the data factory where you can register data factory in Microsoft purview number two run your pipelines in data factory you can create pipelines copy activities data flow activities in the data factory the data is captured automatically during activities execution monitor your data lineage reporting status you can use the lineage status button to access the pipeline monitoring view #4 examine data mining ship information in Microsoft purview account you can browse assets and check the lineage tab to view the copy activity the data flow activity or the execute SSIS package activities 

next let's examine how to browse and search metadata in the Microsoft purview data catalog the Microsoft purview data catalog allows for quick finding of metadata through the browse and search features and browsing the Microsoft preview data catalog helps explore available data either by collection or through traversing the hierarchy of each source in the catalog with searching the Microsoft purview data catalog helps explore the data when you know what you're searching for and add keywords for your search in the search bar the search history and the assets recently accessed in the data catalog allow for a quick pickup from the previous data explored The search results are filtered through the purview relevance engine now test your knowledge by answering some practice questions it's important to remember that the upcoming questions are a bit different from the ones on the actual exam these questions are meant to serve as a review of the topics we discussed add to help give you an idea of the level of knowledge you need for the exam remember that the real exam is about applying the knowledge so the questions on the exam will often give your scenario and ask you what you would do for a closer experience to the actual exam I strongly encourage you to check out the official practice test question one which of the following allows you to repartitions containing segmented streaming data in parallel is it #1 the seed will pool #2 Yvette hubbs #3 as your BLOB storage important #4 Microsoft perfume let's review some answers in this case the answer is event hubs event hubs use partitions that help to segment data and allow you to read each partition in parallel question 2 while implementing Azure synapse analytics database templates what should be done after configuring the like database is it #1 queries data #2 train the machine learning models #3 ingest data to like database or #4 create a database let's review the answer the correct answer is to ingest the data into the late database so how did you do let's now recap what we have covered in this video so far in this functional group we have looked at implementing A partitioning strategy for files analytical workloads streaming workloads and for Azure synapse analytics we have also looked at identifying when partitioning is needed in Azure data lake storage Gen. 2 we've looked at creating and executing queries by using a computer solution that leverages SQL serverless pools and spark clusters we looked at implementing Azure synapse analytics database templates and recommending Azure synapse analytics database templates pushing new or updated data lineage to Microsoft per view and browsing and searching metadata in Microsoft purview data catalog with that we come to the end of functional Group One we hope you will join us for functional group 2 develop data process

## Develop data processing (40–45%)
welcome back my name is Steve Howard and I'm a Microsoft technical trainer this is a DP203 exam prep video series and in this second video we will review how to develop data processing in this group we will revisit how to ingest and transform data develop a batch processing solution develop a strain processing solution and manage batches and pipeline let's begin with the first topic in this functional group TCP runs insert update and delete operations on a target table from the results of a July with a source table so in this we can use the merge to perform such things as insert and update operations on a table in a single statement we could do updated delete operations on a single table in a single statement we could do updated insert operations on a target table by using a derived source table or an external polybase table and we could do insert or update on a target edge table and a graph database we can also insert the results of the merge statement into another table note that polybase is a feature of SQL Server and Azure synapse analytics that enables you to run transact SQL queries that redacta from external data sources it makes these external data sources appear like tables in a SQL database some of the key activities to transform data are as follows a script activity which allows the user to insert modify or a stored procedure activity which enables the user to invoke a stored procedure in Azure SQL database Azure synapse analytics or in a SQL Server database 

next let's examine how to ingest and transform data by using Azure synapse pipelines or Azure data factory performed data transformations with Azure synapse pipelines code free by using the mapping data flow task once you've moved the data into Azure data lake storage Gen. 2 you're ready to build a mapping data flow to transform your data at scale via an Azure data factory or Azure synapse pipeline and then load it into data warehouse the tasks at a high level would include preparing the environment which would include turning on the data flow debug and adding a data flow activity you could also add a data source use the mapping data flow transformation and following the steps to do that writing to a data sync because our data flows will always go from a source to a sink and then after it's built you always want to test by running the pipeline customers can ingest data using Azure data factory as it empowers customers to do code free ETL or ELT including preparation and transformation as your synapse can be used exclusively and it works well for Greenfield projects but for organizations with the existing investments in Azure with Azure data factory Azure databricks and power BI you could take a hybrid approach and combine them with Azure synapse analytics you want to manage your source data files with such strategies as maintaining a well engineered data lake structure and compressing and optimizing files to minimize IOPS or input output operations and splitting your source files to maximize parallel processing 

next let's examine how to design and implement incremental loads there are different techniques to implement incremental loads using Azure data factory you can load delta data from a source database using a watermark to do this you would define a watermark in your source database load the change data between an old watermark and a new watermark from the source table to the destination the load delta data from a SQL database using the change tracking technology enable and application to identify inserted updated or deleted data you can load new and changed files using a last modified date so with this you would copy the new and changed files to the destination store using the last modified date ADF scans all source files and applies the file filter by their last modified date and only copies the new and updated files since the last time to the destination store or you could load new files using time partition folder or file name so there you would copy the new files in which files or folders have already been timed partitioned with the time slice information as a part of the file or folder name 

next let's examine how to transform data by using Apache spark you could transform data using a spark activity in the Azure data factory and synapse analytics in the same way that you would in spark jobs on spark notebooks the high level steps to do it here you would add a spark activity to the pipeline canvas 

next you would select a new spark activity on the canvas no this is done when a spark activity is not already selected you could select the HDI cluster tab or you could choose a synapse spark tool for synapse pipelines and you can select or create a new linked service to an HD insight cluster to execute the spark activity then you will select the script or the jar tab and you can create a new job a link service to an Azure storage account to host your script 

next let's examine how to transform data by using Azure stream analytics the stream analytics pipeline provides a transformed data flow from input to output and streams analytics a job is a unit of execution a stream analytics job pipeline consists of three parts it will be an input that provides the source of the data stream a transformation query that acts on the input so for example a transformation query could aggregate the data or it can perform a join from 2 inputs and you will have an output that identifies the destination of the transformed data keep in mind that stream analytics as the name of applies is a streaming technology the data ingested from source locations is cleansed normalized and processed for other tests using Apache spark in Azure synapse analytics and databricks the choice of techniques depends on your specific business requirements such as replacing values splitting data pattern matching enumerations and custom transformations 

next let's examine how to handle duplicate data you can easily perform tasks such as data deduplication and null filtering by using code snippets and mapping data flows in a new pipeline and activity within the pipeline you may select the source settings tab add a source transformation and then connect it to one of your data sets the dedupe and null check snippets use generic patterns that leverage data flow schema drift and work with any schema from your data set or with data sets that have no predefined schema 

next let's examine how to handle missing data when reading data from a file based data source Apache spark faces two typical error cases first the files may not be readable so for instance they could be missing and accessible or corrupted second even if the files are processable some records may not be parsable for example to the syntax errors and schema mismatch so in such a case you may need to set up exception handling you can obtain the exception records and files and reasons from the exception logs by setting the data source option bad records path that records path specifies a path to store exception files for recording the information about bad records for CSV and Jason source and bad files for all the file based built-in sources so for example parquet in addition when reading files transient errors like network connection exception Iowa exception and so on may occur these errors are ignored and recorded under the bad records path and spark will continue to run on the tasks 

next let's examine how to handle late arriving data for each incoming event as your stream analytics compares the event time with the arrival time so you can use the late arrival policy and out of order policy to configure how stream analytics processes events based on their event time and arrival time you can also choose to adjust or drop events that are light or out of order based on these policies you can use a Lambda correction strategy to detect and correct the late arriving data in the stream layer and update the serving layer accordingly this way you can avoid reprocessing all of your historical data and reduce the latency and cost of your data pipeline you can use different methods of handling later arriving to mint on data lake house using Azure synapse such as deleting fact rows with unmatched dimension keys or moving them to a separate landing table or generating dummy the mansion rose 

next let's examine how to split data that a flows are available both in Azure data factory and Azure synapse pipelines you can use conditional split transformations while mapping your data flows in the conditional split you will see that you will set up a stream name which will be just the name of how we are going to send the data that matches this particular condition you can set up one or multiple conditions to send data based on values in that data two different outputs 

next let's examine how to shred JSON queries can be used to read JSON files using the openrowset function but remember open row set is available only in the serverless SQL pool in synapse and not in the dedicated SQL pool to read Jason documents you can use a couple of different methods you can provide the file URL to the open row set function or you can use the open JSON bulk which will read the contents of the file and return it in a bulb column to read classy JSON files you would set the values 0X0B for the row Terminator to parse Jason documents you will use one of two TC SQL functions you can use the JSON value to return a single scalar value with a valid JSON path where you can use the function open JSON to return a table with one or more columns and rows from a Jason string 

next let's examine how to encode and decode data expressions and functions supported by Azure data factory and Azure synapse analytics in mapping data flows include aski which returns at numeric value of an input character char which returns the ASCII character represented by the input number decode which decodes the encoded input data into a string based on a given character set and encode which encodes the input string data into binary based on a character set 

next let's examine how to configure error handling for a transformation we always must be able to handle error conditions that may arise within our data this could include such things as column translation what we need to do is set the sync error row handling to continue on error when processing database data in such a case we'll choose the error row handling option in the sync transformation and we'll set our output error rows this will automatically generate a CSV file output of your row data along with the driver reported error messages you should also provide logging of column said to not fit into a target string column allowing your data flow to continue you can add a conditional split transformation to handle it especially if you're able to detect it early log the rows that failed split off error rows to avoid the sequel truncation errors and put those entries into a log the extended Apache spark history server helps to debug and diagnose completed and running spark applications so let's revisit how to normalize and denormalize values normalizing is the process of organizing the data in a relational database to reduce redundancy and improve data integrity it involves several steps such as identifying the purpose and scope of the database defining the entities and attributes that will be stored in it then we will apply a series of normal forms first normal form second normal form third normal form and sometimes additional higher normal forms such as voice cod may also be used we typically use normalized forms in our transaction processing but when we move into analytical processing we typically will talk about denormalization denormalization is a process of reversing or relaxing the normalization of relational databases to improve performance or usability it will involve such things as identifying the queries or operations that are frequently performed on the database and the tables or attributes that are involved in them then we want to apply denormalization techniques such as adding redundant data combining tables creating derived attributes or using precomputed summaries in order to reduce the number of job points the number of aggregations or of calculations that are needed for those queries or operations and always when we perform such operations we want to evaluate the impact that we have had so we will leave valuate the impact of denormalization on the storage space on the data integrity and on the maintenance of the database and ensure that the benefits outweigh the cost 

next let's examine how to perform exploratory data analysis exploratory data analysis includes data acquisition data exploration and data evaluation so a data acquisition we want to collect or obtain the data from our various sources such as databases miles API web scraping surveys and so on it's going to involve checking the quality and the quantity and the format of the data and sharing that it meets the requirements and objectives of the analysis from there we moved to expiration and this step involves exploring and visualizing the data using descriptive statistics graphs charts tables and so on it also involves identifying and handling any issues or anomalies in the data such as missing values outliers errors duplicates etcetera it will also involve transforming or manipulating the data to create new features or variables that can enhance the analysis data evaluation involves evaluating and interpreting the results of the data exploration and drawing conclusions or insights from them it also involves communicating and presenting the findings to the stakeholders for audiences using clear and concise language and visuals this will involve identifying any limitations or assumptions of the analysis and suggesting any further steps or action that can be taken based on the findings 

next let's examine how to develop batch processing solutions by using Azure data lake storage has your data breaks as your synapse analytics or ADF a modern data warehouse enables you to collate all of your data at scale easily so you can get to the insides through analytics dashboards operational reporting or advanced analytics for your users the process of building a modern data warehouse will involve ingesting and preparing data and making the data ready for consumption by analytical tools for ingesting at the foundation customers need to build a data lake to store all their data in different data types with Azure data lake storage Gen. 2 or other such storage technologies customers can ingest data using data factory or synapse pipelines this will empower customers to do code free ETL or ELT including preparation and transformation another option available to customers for data preparation would be Azure data breaks for making the data ready as your synapse analytics is present at the heart of the modern data warehouse it is a cloud scale analytical solution that implements a data warehouse using dedicated SQL pool that leverages the massive parallel processing architecture engine to bring together enterprise data warehousing and big data analytics so Azure synapse can be used exclusively and it works well for Greenfield projects but for organizations with existing investments in Azure with Azure data factory with Azure data bricks you can take a hybrid approach and combine them with Azure synapse analytics a key requirement of batch processing is the ability to scale out computations in order to handle a large volume of data we have various technological choices that are available for batch processing and you can choose the solution that's most suitable for your organization's needs 

next let's examine how to use polybase to load data to a SQL pool and configure batch size remember Tia aspects to load your data using polybase with these basic steps that you see here you can use polybase for extracting from loading and for transforming your data you should be familiar with TCL scripts to process using polybase but all of our other technologies such as our Azure data factory and our synapse pipelines as well as using the olap connectors from our spark pools will use polybase for any of the ingestion work so use polybase then as the fastest way to ingest data into the dedicated SQL pool but it is also a way that you can leave the data where it is located in the Azure data lake storage and query that interactively 

next let's examine how to implement Azure synapse link and query the replicated data Azure synapse link allows you to interact with a cosmos DB or with an Azure SQL database from your synapse analytics serverless SQL pool or your dedicated SQL pool or even your spark SQL pools the process to set up link is a little different with each of the technologies that you're linking to some of the key things to remember is that when you are creating an Azure synapse link with a container in cosmos DB the analytical store remains in cosmos DB when you set it up with a SQL Server 2022 or later or with an Azure SQL database then the data will be replicated and stored in the analytical store in the dedicated sequel pool and synapse analytics 

next let's examine how to create data pipelines remember the key steps as shown here to create pipelines these pipelines as we're looking at here are created within the Azure synapse analytics you will create them in synapse studio you can add and configure activities and as you see on the slide here the data flow on the design surface is an activity you will specify new or existing data sets had you will connect your activities to define a processing flow 

next let's examine how to integrate Jupiter or Python notebooks into a data pipeline there are three steps to integrate a Jupiter or Python notebook into a data pipeline from the notebook page you're gonna select select existing or new pipeline from the ready page you will select toggle parameter cell in the settings tab and then you will add the relevant values so as you look at the steps that we have here add the notebook to the new or existing pipeline you're going to designate the cells within your notebook here as a parameter cell which you will see here at the toggle the parameter cell and inside of the parameter cell you will define variables and default values now you will assign the parameter values in your pipeline and with that you're going to specify what the names are and what the values are that you will pass in to your parameters in your notebook activity and this is how you're able to reuse your notebooks with different values to affect the control or the flow or the processing within your notebook 

next let's examine how to upsert data in batch processing upsert activities modify existing records in a target table or they insert new records if no record exists there so we have two major approaches to the absurd of data now for a small number of records you can in fact iterate on a data set of an application we've done this for a long time for every row you can invoke a stored procedure to execute an insert or update or to execute a merge operation this is not necessarily recommended for large data on a dedicated SQL pool we have much better ways that we want to use to accomplish this with that for a large number of records we would leverage bulk insert techniques to upload the entire data set to an Azure SQL database when that's what we're talking about we can execute all the insert or update or merge operations within a single batch to minimize round trips and log rides and maximize throughputs we would use a variation of this for the dedicated SQL pool we want to write our data into BLOB storage or Azure data lake storage we now want to have a polybase table and with the polybase table we can execute a merge statement which is a TCL statement with one pass through the data we can now insert new data or modify the existing rows as our needs may be 

next let's examine how to revert data to a previous state with a dedicated SQL pool we have two types of restore points automatically user defines restore points snapshots creates restore points however the dedicated SQL pool must be in an active state for restore point creation dedicated SQL pool supports an 8 hour recovery point objective you can restore your data warehouse in the primary region from any one of the snapshots taken in the past seven days user defined restore points enables you to manually trigger snapshots to create restore points of your data warehouse before and after large modification user defined restore points are available for seven days and are automatically deleted on your behalf you cannot change the retention period of the user defined restore points 42 user defined restore points are guaranteed at any point in time so they must be deleted before creating another one 

next let's examine how to read from and write to a delta lake delta lake brings atomicity consistency isolation and durability that is acid transactions to Apache synapse spark pools and to databricks you would use the spark dot read dot format delta method to load a delta table from a given path or use that delta dot table .4 path method to create a delta table object from a given path you would use standard spark data frames or SQL APIs to query or manipulate the data in a delta table you can also use the delta table API to perform delta specific operations such as history merge update delete etcetera to use a pend or overwrite mode with the delta lake you need to follow a few steps you need to specify the format as delta when writing data to a delta lake table using the data frame or SQL API so for example data dot write dot format and in parentheses delta and then dot say you can specify the mode as append or overwrite depending on whether you want to add new data to an existing delta table or replace the existing data with new data so for example data dot write dot format and delta then you will put mode and append optionally you can use the replace where option to selectively override only the data that matches a given predicate 

next let's examine how to create a stream processing solution by using stream analytics and Azure event hubs the reference architectures here show end to end stream processing pipelines with data bricks and also with stream analytics for reference architecture with databricks the pipeline ingest data from two data sources it performs a join on related records from each stream enriches the results and calculates an average in real time the results are stored for further analysis so for data sources in this architecture there are two data sources that generate data streams in real time our Azure event hubs is an event ingestion service this architecture uses two event hubs instances one for each data source each data source sends a stream of data to the associated event hubs for Azure data bricks this is an Apache spark based analytics platform optimized for the Microsoft Azure cloud services platform the output from Azure databricks job is a series of records which are going to be written into cosmos DB and this particular use case using the Cassandra API this API is used because it supports time series data model and then we will use our data analysis tools such as power BI for data visualization now Azure log analytics can't collect the data from our Azure monitor and stored in a log analytics workspace log analytics queries can be used to analyze and visualize metrics and inspect log messages to identify issues within the application similarly the second diagram shows the reference architecture with stream analytics 

next let's examine how to process data by using spark structured streaming you can access Azure synapse from Azure databricks using give me Azure synapse connector a data source implementation for Apache spark that uses Azure BLOB storage and polybase or the copy statement and Azure synapse to transfer large volumes of data efficiently between an Azure databricks cluster and an Azure synapse instance the spark driver can't connect to an Azure snaps using JDBC with the username and password or oauth 2.0 with a service principal front that indication spark driver and executors to Azure storage account acts as an intermediary to store bulk data when reading from or writing to Azure snaps spark connects to the storage container using one of the built-in connectors the Azure BLOB storage or Azure data lake storage Gen. 2 after you set up an account key and secret for the storage account you can set the forward spark as your storage credentials to true in which case as your snaps connector automatically discovers the account access key set in the notebook session configuration or the global Hadoop configuration and forwards the storage account access key to the connected Azure synapse instance by creating a temporary Azure database scoped credential 

next let's examine how to create windowed aggregates stream analytics provides native support for windowing functions that helps other complex stream processing jobs with minimal efforts over these 

next two slides we will show five kinds of temporal windows tumbling hopping sliding session and snapshot you should be familiar with what each of these are as well as the sequel to create these window types for each type of aggregation so you will see here for a tumbling window that we that each window begins where the previous window ended there will be no duplicates from one window to the other and the output is produced only at the end of the window a hoppy window you will specify a different time for your output and the amount of time over which you will aggregate data and so you can see the example here of a 10 second hopping window with a 5 second hop which means we will report output every five seconds and we will aggregate over the previous 10 seconds a sliding window generates output anytime the contents of the window changes so if data goes out of scope of the window we will produce output if new data enters the window it will produce output a session window allows you to tell the count of tweets for example that occur within 5 seconds of each other and a snapshot just looks at any particular point in time 

next let's examine how to handle schema drift schema draft is a case where your sources often change the metadata things such as fields columns and types can be added they can be removed but they can be changed on the fly without handlings for schema drift your data flow becomes vulnerable to upstream data source changes so 2 types of schema drift handling are you can handle it at the source or you can handle it at the sink when you're handling it at the source columns coming into your data flow from your source definition are defined as drifted when they're not present in your source projection or when you select the data set for your source ADF for example will automatically take the schema from the data set and create a projection from that data set schema definition so to enable schema drift you will select the allow schema drift checkbox in your source transformation if you handle it at the sink that the sync transformation schema drift is where you're going to write additional columns on top of what is already defined in the sync data schema so to enable schema drift with select allow schema drift in your sink transformation schema drift is enabled and sure the automapping slider on the mapping tab is turned on when your data flow has drifted you can access them in your transformations by using the by position or by name expressions to explicitly reference a column either by its relative position or by the name within the projection you can add a column pattern in the derived column or aggregate transformation to match on any combination of name stream position origin or type and then add a rules based mapping and the select or sync transformation to match drifted columns to columns aliases or other via a pattern 

next let's examine how to process time series data time series data is a set of values organized as per time a key characteristic of time series data is temporal ordering that organizes events and the order in which they occur and arrive for processing time series based systems such as Internet of Things capture data in real time by using a real time processing architecture Azure IoT hub Azure event hubs for Kafka on HD insight and just data from one or more data sources end to stream processing layer the stream processing layer processes the data to hand off the process data to a machine learning service for predictive analytics or other such downstream services and analytical data stores such as Azure data explorer H base Azure cosmos TB or Azure data lake stores the process data and analytics and reporting application or service such as power BI or open TSDB for H base display the time series data for analysis 

next let's examine how to process data across partitions you can scale stream analytics jobs by configuring input partitions and tuning the analytics query definition stream analytics jobs can consume and write different partitions in parallel which increases throughput you can use repartitioning to scale your Azure stream analytics query for scenarios that can't be fully parallelized a stream analytics job can consume and write different partitions in parallel which increases the throughput and embarrassingly parallel job is the most scalable scenario in Azure stream analytics it connects 1 partition of the input to 1 instance of the query to 1 partition in the output some examples of partition values that allow a fully paralleled job might be 8 event hub input partitions and eight event hub output partitions eight event hub input partitions and BLOB storage output or eight event hub input partitions and BLOB storage output partitioned by a custom field with arbitrary cardinality or 8 BLOB storage and put partitions and BLOB storage output or 8 ball storage input partitions and eight event hub output partitions repartitioning or reshuffling is required when you process data on a stream that's not sharded according to the natural input scheme such as a petition ID for event hubs when you repartition each Shard can be processed independently which allows you to linearly scale out your streaming pipeline the two ways to repetition your inputs include using a separate stream analytic Alex job that does the repartitioning so you can create a job that reads input and writes to an event hub output using a partition key this event hub can then serve as input for another stream analytics job where you implement your analytics logic or you can use a single job but do the repartitioning first before your custom analytics login you can also introduce a step in your query that first repartitions the input and this can then be used by other steps in your query so partitioning lets you divide data into subsets based on a partition key if your input like maybe event hubs for example is partitioned by a key it's highly recommended to specify this partition key when adding the input to your stream analytics job all Azure stream analytics input can take advantage of partitioning so when you work with stream analytics you can take advantage of partitioning in the outputs as well so 

next let's revisit how to configure watermarking during processing structured streaming uses watermarks to control the threshold for how long to continue processing updates for a given state entity such as aggregations over a time window or unique keys and a join between 2 streams you should be familiar with using watermarks in order to prevent long running queries or data built up in your structured spark streaming jobs 

next let's examine how to scale resources streaming units and parallelized jobs can help you scale jobs and resources or unique keys in a join between 2 streams you should be familiar with using watermarks in order to prevent long running queries or data buildup and your structured spark streaming jobs 

next let's examine how to scale resources straining your Nets and parallelize jobs can help you scale jobs and resources streaming units are the computing resources allocated to execute a stream analytics job they can help you scale jobs as shown on the slide you can choose the manual scale and set the streaming units or choose the custom auto scale method or the streaming units will be set by the system a stream analytics job definition is made-up of at least one streaming input a query and an output partitions in these inputs and outputs can help divide data into subsets based on a partitioning key scaling a stream analytics job leverages these partitions in the input and output a stream analytics job can consume and write different partitions in parallel which will increase the throughput 

next let's examine how to create tests for data pipelines in batch and stream processing as your stream analytics tools for Visual Studio lets you test jobs locally from the IDE using live event streams from IoT hub Azure event hub and BLOB storage so the high level steps to create tests for data pipelines are running your queries on automatically sampled incoming data the Azure stream analytics automatically provides events from this streaming inputs you can run queries in the default sample or set time frames for this sample you can running queries on sample data uploaded from a local file local data can be used instead of live data to test your Azure stream analytics query and then troubleshooting errors and input and query size 

next let's examine how to optimize pipelines for analytical or transactional purposes you could use the repartitioning and parallelization to optimize processing with Azure stream analytics it allows each Shard to be processed independently to linearly scale out streaming pipelines observe and experiment the resource use each of your job to determine the exact number of partitions needed you must adjust the number of streaming units according to physical units needed for each partition use explicit repartitioning to match the optimal partition count to maximize throughput when your job uses SQL database for output a stream analytics job consumes and writes different partitions in parallel to increase throughput for a job to be parallel partitioning keys must be aligned between all inputs all query logic steps and all outputs 

next let's examine how to handle interruptions output data error handling policies apply only to data conversion errors that occur when the output event produced by a stream analytics job does not conform to the schema of the target scene you can configure this policy by choosing either retry or drop with retry when an error occurs as your stream analytics retries writing the event indefinitely until the right succeeds there's no timeout for retries eventually all subsequent events are blocked from processing by the event that is retrying this option is the default output error handling process drop well tell Azure stream analytics to drop any output event that results in a data conversion error the dropped events cannot be recovered for reprocessing later 

next let's examine how to configure exception handling in batch and stream processing you can use a fail activity in a pipeline to throw an error in a pipeline intentionally and customize both its error message and error codes to do that create a fail activity with the UI and ensure you understand the fail activity error message and code so to use a fail activity you're gonna need disperse search for and find fail in the pipeline activities pane and drag the fail activity to the pipeline cameras you'll select this activity on the canvas and on the settings tab you'll edit its details there you will enter a message and an error code these can be literal string expressions or any combination of dynamic expressions functions system variables and so on ensure you're aware of the error messages and error codes they're often set by the developer but some are also set by ADF 

next let's examine how to upsert data in stream processing stream analytics integration with Azure cosmos DB allows you to insert or update records in your container based on a given document ID column this is also called an upsert stream analytics uses an optimistic upsert approach updates happen only when an insert fails with a document ID conflict with compatibility level 1.0 stream analytics performs these updates as a patch operation so it it enables partial updates to the document with compatibility level 1.2 upsert behaviorist modified to insert or replace the document stream analytics supports native integration to bulk right into Azure cosmos DB this integration enables writing effectively to Azure cosmos DB while maximizing throughput and efficiently handling throttling requests 

next let's examine how to replay archived stream data as your stream analytics allows checkpoint and replay features which may have an impact on job recovery the replay ketchup time estimates the length of the delay due to a service upgrade to estimate the replay catch up time with load the input load them and put event hubs with enough data to cover the largest window size in your query at expected event rates note that the event timestamp should be close to the wall clock time throughout that time period similar to live input feed second she would start the job 3rd you will measure the time between the start time and the time when the first output is generated this time is estimated to lay the job would incur during a service upgrade before if necessary partition your job and increase the number of S years to spread the load out to more nodes if the time is too long you can also reduce the window sizes of your query and perform further aggregations or other stateful processing on the output 

next let's examine how to trigger batches select the pipeline and click the add trigger and the new click new again and in the box it appears access the new trigger dialog box these are the steps to add a trigger to a synapse pipeline triggers are anything that will cause the pipeline to execute you can create triggers based on a time schedule or based on any event that you can fire from any service inside of Azure this provides an elegant way for you to be able to process data according to your business requirements 

next let's examine how to handle failed batch loads comprehensive error checking can help you identify and diagnose issues that occur in background operations such as pool and node failures pool and node failures can occur in the background operations that need to be to take didn't avoid it so for pool errors these could be due to resize timeout or failure automatic scaling failure or pool deletion failure some issues and their fixes might include a resized timeout insufficient core quota insufficient subnet IP's and insufficient resources automatic scaling failures to be to catch the issue and use the autoscale run property to get information on any errors have pulled to leash and failures you would take action according to the issues there are also a number of other node errors you might need to detect such as start task failures application package container download failures node OS updates and so on since caesar's can occur even when the batch successfully allocates nodes in a pool and render all the nodes unusable it's important to know the status of the nodes being used and which are usable to evoid incurring wasted cost and have a backup for relevant objects such as containers applications packages and start tasks 

next let's examine how to validate batch loads there are three ways to validate batch loads subscription level batch account level and batch resource was subscription level operational event data is collected in several categories by Azure activity log at the subscription level activity log collects events related to account creation deletion and key management for batch accounts at batch account level each bank account it would be validated for example using the Azure monitor feature the Azure monitor feature collects metrics and resource logs for resources within a batch account such as pools jobs and tasks for batch resource validation batch APIs are used to monitor or query the status of resources and batch applications the resources include jobs tasks nodes and pools 

next let's revisit how to manage data pipelines in Azure data factory or Azure synapse the monitoring and management application provides support for data pipelines management and troubleshooting of any issues so to monitor your data pipelines you want to understand pipelines and activity states navigate to the data factory view the state of each activity inside of a pipeline pause and resume pipelines debug pipelines rerun failures in a pipeline create alerts in the Azure portal move data factory to different resource groups or subscriptions 

next let's examine how to schedule data pipelines in Azure data factory or Azure synapse pipelines a scheduled trigger allows you to schedule a pipeline to run periodically this could be hourly daily or so on like this so to schedule data pipelines in Azure data factory or Azure synapse pipelines you would access the edit tab to select the new edit trigger from the ad triggers page you would choose trigger and choose the plus or new from the new triggers page specify the schedule details publish the trigger and switch to the pipeline runs tab refresh the list for pipeline runs one thing to note is that you can create a trigger on a time schedule you can also create a trigger off of any event that can be raised in Azure and that could include a BLOB trigger if you're waiting for the arrival of a file and you need to process the pipeline on that file as soon as it arrives 

next let's examine how to implement version control for pipeline artifacts sadaf studio allows you to associate the synapse workspace with a git repository as your DevOps or GitHub snap studio workspace can be associated with only one git repository at a time you would go to the manage hub of the snap studio select get configuration and the source control if you have no repository connected then you would click configure one thing to remember in all of this is the only way you can save a synapse pipeline or Azure data factory pipeline without connecting to source control is to publish that's not going to be very effective for you so in order to save your progress as you work you need to configure your source control 

next let's examine how to manage spark jobs in a pipeline the spark activity is a data transformation activity by data factory the steps to manage spark jobs in a pipeline are from the data factory page select monitor and manage this starts the monitoring application in another tab reset the start time filter and click apply ensure that the data slice is in ready state now from the activities window section now let's test your knowledge by answering some practice questions you should remember that the upcoming questions are different from the ones on the actual exam these questions are meant to serve as a review of the topics we discussed and to help give you an idea of the level of knowledge you need for the exam remember that the real exam is about applying the knowledge so the questions on the exam will often give you a scenario and ask you what you would do for a closer experience on the actual exam I strongly encourage you to check out the official practice test question one you need to use spark to analyze data in a parquet file what should you do should you want one load the park file into a data frame to import the data into a table and a serverless SQL pool or three convert the data to a CSV format the correct answer is you should load the parquet file into a data frame question 2 you have loaded the spark data frame with data and you now want to use it in a delta like table what format should you use to write the data frame to storage should you use one CSV 2 park or a three delta the correct answer is 3 delta let's recap what we've covered we have looked at transform data by using transact SQL ingest and transform data by using Azure synapse pipelines or ADF designing and implementing incremental loads transforming data by using Apache spark transform data by using Azure stream analytics cleanse data handled duplicate missing and late arriving data split data shred Jason encode and decode data configure error handling for a transformation normalize and denormalize values or performed data exploration analysis we have also looked at developing batch processing solutions by using Azure data lake storage Azure data bricks Azure synapse analytics and ADF use polybase to load to a sequel pool and configure the batch size implement as your synapse link and query the replicated data create data pipelines integrate Jupiter or Python notebooks into a data pipeline upsert data and batch processing configure exception handling and batch processing read from that right to the delta lake create a stream processing solution process data by using spark structured streaming and create windowed aggregates handle schema draft and process time series data process data across partitions and within one partition configured and watermarking during processing scale resources and create tests for data like pipelines and stream processing optimize pipelines for analytical or transactional purposes handle interruptions and configure exception handling and stream processing upsert data and stream processing replay archive stream data trigger batches and handle failed batch loads and validate batch loads manage and schedule data pipelines in ADF or Azure snaps pipelines implement version control for pipeline artifacts and manage spark jobs and a pipeline with that we come to the end of functional group 2 we hope you'll join us for functional Group 3 secure monitor and optimize data storage and data processing
